////////////////////////////////////////////////
//////// An example configuration file ////////
//////////////////////////////////////////////


/* here defined are default configuration settings */
[core]
// the list of modules to use (in order)
modules = nmap.avain_nmap, web.gobuster.avain, cve_correlation.avain_cve_correlation, login_bruteforce.hydra_ssh.avain, login_bruteforce.hydra_telnet.avain
default_trust = 3                                                // the default quality of data / trust level for scan modules
scan_trust_aggr_scheme = TRUST_AGGR                              // possible values --> {TRUST_MAX, TRUST_AGGR}
scan_result_aggr_scheme = MULTIPLE                               // possible value --> {SINGLE, MULTIPLE, FILTER}
DB_expire = 20160                                                // in minutes, i.e. every other week
print_result_types = SCAN, WEBSERVER_MAP                         // the intermediate result types to output


// here defined are module specific configuration settings
[nmap.avain_nmap]
// add_nmap_params = --max-rtt-timeout 100ms --max-retries 1     // additional Nmap params
scan_type = S                                                    // SYN scan and UDP scan require root privileges
fast_scan = False                                                // whether Nmap should use T5 and F option as speedup
add_scripts = default, http-headers, smb-os-discovery, banner    // additional scripts Nmap should use
timing_template = 3                                              // the timing (or aggressiveness) template to use

[cve_correlation.avain_cve_correlation]
DB_expire = 10080                                                // in minutes, i.e. every week
skip_os = False                                                  // whether to skip OS CVE analysis --> {True, False}
max_cve_count = -1                                               // the maximum number of CVEs to retrieve; -1 for unlimited
squash_cpes = True                                               // whether to squash every discovered CPE in case of invalid CPE
allow_versionless_search = True                                  // whether to fully search for CVEs when CPE has no version

[login_bruteforce.hydra_ssh.avain]
wordlists = ../wordlists/mirai_user_pass.txt                     // Mirai wordlist relative to module dir
tasks = 16                                                       // the number of parallel Hydra tasks

[login_bruteforce.hydra_telnet.avain]
wordlists = ../wordlists/mirai_user_pass.txt                     // Mirai wordlist relative to module dir
timeout = 300                                                    // Hydra timeout in seconds (if Telnet bruteforce does not work)
tasks = 16                                                       // the number of parallel Hydra tasks

[web.gobuster.avain]
wordlist = ../wordlists/dirbuster/directory-list-2.3-small.txt   // wordlist to use relative to module dir
extensions = php, html                                           // file extensions to search for
depth = 3                                                        // the search depth
threads = 10                                                     // how many threads to use for gobuster
do_reverse_dns = True                                            // whether the domain name(s) of the IP should be used if it has one
allow_file_depth_search = False                                  // whether to search for directories if the url ends with a file
exclude_dirs = js, css                                           // what directories not to search in
per_directory_timeout = 300                                      // timeout for searching a directory in seconds
per_host_timeout = 1800                                          // timeout for mapping a web server in seconds

[web.crawler.avain]
do_reverse_dns = True                                            // whether to use the domain of a server if it is available
max_depth = 3                                                    // max directory depth for crawling (-1 for unlimited)
user_agent = Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)  // the user agent to use
cookies = {}                                                     // the cookies to use when crawling, specified as JSON object with (key, value) pairs
use_linkfinder = False                                           // whether to use Linkfinder to statically find pages in JS code (false positives likely)
use_selenium = True                                              // whether Selenium should be used to run javascript and trigger events
extract_info_from_forms = True                                   // whether to analyze forms and try to extract GET / POST params or URLs from them
extract_comments = True                                          // if True, try to extract comments from crawled HTML, JS, etc. documents
check_linkfinder = True                                          // whether a URL discovered by linkfinder should be checked for existence via HEAD request
linkfinder_check_timeout = 2                                     // timeout for a checking HEAD request; can significantly impact crawling speed
crawl_parameter_links = True                                     // if True, URLs are uniquely identified by path & GET parameters and crawled accordingly
max_path_visits = 25                                             // how many times a path can be crawled with different GET parameters
